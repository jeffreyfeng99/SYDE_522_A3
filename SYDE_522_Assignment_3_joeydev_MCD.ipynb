{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SYDE-522_Assignment_3_joeydev_MCD.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOrGO+RU8jJyaBlBeaei4J8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeffreyfeng99/SYDE_522_A3/blob/master/SYDE_522_Assignment_3_joeydev_MCD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.utils.data as data\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.autograd import Variable\n",
        "from torch.autograd import Function\n",
        "\n",
        "from PIL import Image\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "M9VQaWUFPd_D"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lz7QvnqDPjnJ",
        "outputId": "f39d747d-f012-4d7c-b43d-79e3ee5e67db"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cuda = True\n",
        "cudnn.benchmark = True\n",
        "\n",
        "model_name = \"resnet18\"\n",
        "optim_name = \"momentum\"\n",
        "\n",
        "LR = 1e-3\n",
        "MOMENTUM = 0.9\n",
        "BATCH_SIZE = 32\n",
        "IMAGE_SIZE = 224 #227\n",
        "# FT_OUT_SIZE = 512\n",
        "N_EPOCH = 50\n",
        "\n",
        "AUGMENT = False\n",
        "N_K = 4 # how many steps to repeat the generator update?\n",
        "N_LAYER = 2 # how many layers for classifier\n",
        "LOG_INTERVAL = 50\n"
      ],
      "metadata": {
        "id": "cm9pIM1sPcNc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0-jDAJQ1PWNc"
      },
      "outputs": [],
      "source": [
        "dataset_root = '/content/drive/MyDrive/4B/SYDE-522/data'\n",
        "output_root = '/content/drive/MyDrive/4B/SYDE-522/submission/03302022_'+model_name\n",
        "source_dataset_name = 'train_set'\n",
        "target_dataset_name = 'test_set'\n",
        "\n",
        "source_image_root = os.path.join(dataset_root, source_dataset_name)\n",
        "target_image_root = os.path.join(dataset_root, target_dataset_name)\n",
        "\n",
        "train_path = os.path.join(source_image_root, 'train_set')\n",
        "val_path = os.path.join(target_image_root, 'test_set')\n",
        "\n",
        "train_label_list = os.path.join(dataset_root, 'train_labels.csv')\n",
        "\n",
        "os.makedirs(output_root, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GradReverse(Function):\n",
        "    def __init__(self, lambd):\n",
        "        self.lambd = lambd\n",
        "    def forward(self, x):\n",
        "        return x.view_as(x)\n",
        "    def backward(self, grad_output):\n",
        "        return (grad_output*-self.lambd)\n",
        "\n",
        "def grad_reverse(x,lambd=1.0):\n",
        "    return GradReverse(lambd)(x)\n",
        "\n",
        "class ResBase(nn.Module):\n",
        "    def __init__(self,option='resnet18',pret=True):\n",
        "        super(ResBase, self).__init__()\n",
        "        self.dim = 2048\n",
        "        if option == 'resnet18':\n",
        "            model_ft = models.resnet18(pretrained=pret)\n",
        "            self.dim = 512\n",
        "        if option == 'resnet50':\n",
        "            model_ft = models.resnet50(pretrained=pret)\n",
        "        if option == 'resnet101':\n",
        "            model_ft = models.resnet101(pretrained=pret)\n",
        "        if option == 'resnet152':\n",
        "            model_ft = models.resnet152(pretrained=pret)\n",
        "        mod = list(model_ft.children())\n",
        "        mod.pop()\n",
        "        #self.model_ft =model_ft\n",
        "        self.features = nn.Sequential(*mod)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), self.dim)\n",
        "        return x\n",
        "\n",
        "class ResClassifier(nn.Module):\n",
        "    def __init__(self, num_classes=7, num_layer=2, num_unit=2048, prob=0.5, middle=1000):\n",
        "        super(ResClassifier, self).__init__()\n",
        "        layers = []\n",
        "        # currently 10000 units\n",
        "        layers.append(nn.Dropout(p=prob))\n",
        "        layers.append(nn.Linear(num_unit,middle))\n",
        "        layers.append(nn.BatchNorm1d(middle,affine=True))\n",
        "        layers.append(nn.ReLU(inplace=True))\n",
        "\n",
        "        for i in range(num_layer-1):\n",
        "            layers.append(nn.Dropout(p=prob))\n",
        "            layers.append(nn.Linear(middle,middle))\n",
        "            layers.append(nn.BatchNorm1d(middle,affine=True))\n",
        "            layers.append(nn.ReLU(inplace=True))\n",
        "        layers.append(nn.Linear(middle,num_classes))\n",
        "        self.classifier = nn.Sequential(*layers)\n",
        "\n",
        "        #self.classifier = nn.Sequential(\n",
        "        #    nn.Dropout(),\n",
        "        #    nn.Linear(2048, 1000),\n",
        "        #    nn.BatchNorm1d(1000,affine=True),\n",
        "        #    nn.ReLU(inplace=True),\n",
        "        #    nn.Dropout(),\n",
        "        #    nn.Linear(1000, 1000),\n",
        "        #    nn.BatchNorm1d(1000,affine=True),\n",
        "        #    nn.ReLU(inplace=True),\n",
        "        #    nn.Linear(1000, num_classes),\n",
        "\n",
        "    def set_lambda(self, lambd):\n",
        "        self.lambd = lambd\n",
        "    def forward(self, x,reverse=False):\n",
        "        if reverse:\n",
        "            x = grad_reverse(x, self.lambd)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "hPPa_PHQQbJb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GetLoader(data.Dataset):\n",
        "    def __init__(self, data_root, data_list=None, transform=None):\n",
        "        self.root = data_root\n",
        "        self.transform = transform\n",
        "\n",
        "        # we only pass data_list if it's training set\n",
        "        if data_list is not None:\n",
        "            df = pd.read_csv(data_list)\n",
        "            self.img_paths = df['dir'].to_list()\n",
        "\n",
        "            if 'label2' in df.columns:\n",
        "                self.img_labels = df['label2'].to_list()\n",
        "            else: \n",
        "                self.img_labels = ['0' for i in range(len(self.img_paths))]\n",
        "\n",
        "            if 'label1' in df.columns:\n",
        "                self.domain_labels = df['label1'].to_list()\n",
        "            else: \n",
        "                self.domain_labels = ['0' for i in range(len(self.img_paths))]\n",
        "        else:\n",
        "            # Walk through test folder - we don't need labels\n",
        "            self.img_paths = [f for root,dirs,files in os.walk(data_root) for f in files if f.endswith('.png')]\n",
        "            self.img_labels = ['0' for i in range(len(self.img_paths))]\n",
        "            self.domain_labels = ['0' for i in range(len(self.img_paths))]\n",
        "\n",
        "        self.n_data = len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        img_paths, labels, domain_labels = self.img_paths[item%self.n_data], self.img_labels[item%self.n_data], self.domain_labels[item%self.n_data]\n",
        "        imgs = Image.open(os.path.join(self.root, img_paths)).convert('RGB')\n",
        "\n",
        "        if self.transform is not None:\n",
        "\n",
        "            if isinstance(self.transform, list):\n",
        "                tform = self.transform[int(domain_labels)]\n",
        "            else:\n",
        "                tform = self.transform\n",
        "\n",
        "            imgs = tform(imgs)\n",
        "            labels = int(labels)\n",
        "            domain_labels = int(domain_labels)\n",
        "\n",
        "        return imgs, labels #, domain_labels, img_paths\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_data\n",
        "\n",
        "class PairedData(object):\n",
        "    def __init__(self, data_loader_A, data_loader_B, max_dataset_size, flip):\n",
        "        self.data_loader_A = data_loader_A\n",
        "        self.data_loader_B = data_loader_B\n",
        "        self.stop_A = False\n",
        "        self.stop_B = False\n",
        "        self.max_dataset_size = max_dataset_size\n",
        "        self.flip = flip\n",
        "\n",
        "    def __iter__(self):\n",
        "        self.stop_A = False\n",
        "        self.stop_B = False\n",
        "        self.data_loader_A_iter = iter(self.data_loader_A)\n",
        "        self.data_loader_B_iter = iter(self.data_loader_B)\n",
        "        self.iter = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        A, A_paths = None, None\n",
        "        B, B_paths = None, None\n",
        "        try:\n",
        "            A, A_paths = next(self.data_loader_A_iter)\n",
        "        except StopIteration:\n",
        "            if A is None or A_paths is None:\n",
        "                self.stop_A = True\n",
        "                self.data_loader_A_iter = iter(self.data_loader_A)\n",
        "                A, A_paths = next(self.data_loader_A_iter)\n",
        "\n",
        "        try:\n",
        "            B, B_paths = next(self.data_loader_B_iter)\n",
        "        except StopIteration:\n",
        "            if B is None or B_paths is None:\n",
        "                self.stop_B = True\n",
        "                self.data_loader_B_iter = iter(self.data_loader_B)\n",
        "                B, B_paths = next(self.data_loader_B_iter)\n",
        "\n",
        "        if (self.stop_A and self.stop_B) or self.iter > self.max_dataset_size:\n",
        "            self.stop_A = False\n",
        "            self.stop_B = False\n",
        "            raise StopIteration()\n",
        "        else:\n",
        "            self.iter += 1\n",
        "            if self.flip and random.random() < 0.5:\n",
        "                idx = [i for i in range(A.size(3) - 1, -1, -1)]\n",
        "                idx = torch.LongTensor(idx)\n",
        "                A = A.index_select(3, idx)\n",
        "                B = B.index_select(3, idx)\n",
        "            return {'S': A, 'S_label': A_paths, # TODO: prob not exactly what we want\n",
        "                    'T': B, 'T_label': B_paths}\n",
        "\n",
        "class CVDataLoader(object):\n",
        "    def initialize(self, dataset_A,dataset_B,batch_size,shuffle=True):\n",
        "        #normalize = transforms.Normalize(mean=mean_im,std=std_im)\n",
        "        self.max_dataset_size = float(\"inf\")\n",
        "        data_loader_A = torch.utils.data.DataLoader(\n",
        "            dataset_A,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=shuffle,\n",
        "            num_workers=4)\n",
        "        data_loader_B = torch.utils.data.DataLoader(\n",
        "            dataset_B,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=shuffle,\n",
        "            num_workers=4)\n",
        "        self.dataset_A = dataset_A\n",
        "        self.dataset_B = dataset_B\n",
        "        flip = False\n",
        "        self.paired_data = PairedData(data_loader_A, data_loader_B, self.max_dataset_size, flip)\n",
        "\n",
        "    def name(self):\n",
        "        return 'UnalignedDataLoader'\n",
        "\n",
        "    def load_data(self):\n",
        "        return self.paired_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return min(max(len(self.dataset_A), len(self.dataset_B)), self.opt.max_dataset_size)\n"
      ],
      "metadata": {
        "id": "VdXwNYiojYOV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_fn(mu=(0.6399, 0.6076, 0.5603), std=(0.3065, 0.3082, 0.3353), augment=False):\n",
        "    if augment:\n",
        "        # I guess we will also try augmentation\n",
        "        img_transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.CenterCrop(IMAGE_SIZE),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=mu, std=std)\n",
        "        ])\n",
        "    else:\n",
        "        img_transform = transforms.Compose([\n",
        "          transforms.Resize(IMAGE_SIZE),\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize(mean=mu, std=std) \n",
        "        ])\n",
        "\n",
        "    return img_transform\n",
        "\n",
        "def prep_dataloader(trds, vlds, shuffle=False):\n",
        "    dataloader = CVDataLoader()\n",
        "    dataloader.initialize(trds, vlds, BATCH_SIZE, shuffle=shuffle)\n",
        "    dataset = dataloader.load_data()\n",
        "    \n",
        "    return dataloader, dataset"
      ],
      "metadata": {
        "id": "oWEg2-CVPbM5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_transforms = {\n",
        "    train_path: preprocess_fn(augment=True),\n",
        "    val_path: preprocess_fn(augment=True)\n",
        "}\n",
        "\n",
        "\n",
        "datasets = {\n",
        "        train_path: GetLoader(\n",
        "                data_root=train_path,\n",
        "                data_list=train_label_list,\n",
        "                transform=data_transforms[train_path]),\n",
        "        val_path: GetLoader(\n",
        "            data_root=val_path,\n",
        "            transform=data_transforms[val_path])\n",
        "        }\n",
        "\n",
        "train_loader, dataset = prep_dataloader(datasets[train_path], datasets[val_path])\n",
        "test_loader, dataset_test = prep_dataloader(datasets[train_path], datasets[val_path], shuffle=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xc5-FXNXPs4V",
        "outputId": "4ab4720f-4ffc-48c2-9998-c8b6746b2d0d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data_loader_A = torch.utils.data.DataLoader(\n",
        "#             datasets[train_path],\n",
        "#             batch_size=BATCH_SIZE,\n",
        "#             shuffle=False,\n",
        "#             num_workers=4)\n",
        "# data_loader_A_iter = iter(data_loader_A)"
      ],
      "metadata": {
        "id": "JC-1Q-muWOPf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_gpu = torch.cuda.is_available()\n",
        "manual_seed = random.randint(1, 10000)\n",
        "random.seed(manual_seed)\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(manual_seed)\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.01)\n",
        "        m.bias.data.normal_(0.0, 0.01)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.01)\n",
        "        m.bias.data.fill_(0)\n",
        "    elif classname.find('Linear') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.01)\n",
        "        m.bias.data.normal_(0.0, 0.01)\n",
        "        \n",
        "\n",
        "G = ResBase(model_name)\n",
        "F1 = ResClassifier(num_layer=N_LAYER)\n",
        "F2 = ResClassifier(num_layer=N_LAYER)\n",
        "F1.apply(weights_init)\n",
        "F2.apply(weights_init)\n",
        "\n",
        "if cuda:\n",
        "    G.cuda()\n",
        "    F1.cuda()\n",
        "    F2.cuda()\n",
        "if optim_name == 'momentum':\n",
        "    optimizer_g = optim.SGD(list(G.features.parameters()), lr=LR, weight_decay=0.0005)\n",
        "    optimizer_f = optim.SGD(list(F1.parameters())+list(F2.parameters()), \n",
        "                            momentum=MOMENTUM, lr=LR, weight_decay=0.0005)\n",
        "elif optim_name == 'adam':\n",
        "    optimizer_g = optim.Adam(G.features.parameters(), lr=LR,weight_decay=0.0005)\n",
        "    optimizer_f = optim.Adam(list(F1.parameters())+list(F2.parameters()), \n",
        "                             lr=LR, weight_decay=0.0005)\n",
        "else:\n",
        "    optimizer_g = optim.Adadelta(G.features.parameters(), lr=LR, weight_decay=0.0005)\n",
        "    optimizer_f = optim.Adadelta(list(F1.parameters())+list(F2.parameters()), \n",
        "                                 lr=LR, weight_decay=0.0005)   "
      ],
      "metadata": {
        "id": "TvJBuiLfjzJW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(num_epoch):\n",
        "    criterion = nn.CrossEntropyLoss().cuda()\n",
        "    for ep in range(num_epoch):\n",
        "        G.train()\n",
        "        F1.train()\n",
        "        F2.train()\n",
        "        for batch_idx, data in enumerate(dataset):\n",
        "            if batch_idx * BATCH_SIZE > 30000:\n",
        "                break\n",
        "            if cuda:\n",
        "                data1 = data['S']\n",
        "                target1 = data['S_label']\n",
        "                data2  = data['T']\n",
        "                target2 = data['T_label']\n",
        "                data1, target1 = data1.cuda(), target1.cuda()\n",
        "                data2, target2 = data2.cuda(), target2.cuda()\n",
        "            # when pretraining network source only\n",
        "            eta = 1.0\n",
        "            data = Variable(torch.cat((data1,data2),0))\n",
        "            target1 = Variable(target1)\n",
        "            # Step A train all networks to minimize loss on source\n",
        "            optimizer_g.zero_grad()\n",
        "            optimizer_f.zero_grad()\n",
        "            output = G(data)\n",
        "            output1 = F1(output)\n",
        "            output2 = F2(output)\n",
        "\n",
        "            output_s1 = output1[:BATCH_SIZE,:]\n",
        "            output_s2 = output2[:BATCH_SIZE,:]\n",
        "            output_t1 = output1[BATCH_SIZE:,:]\n",
        "            output_t2 = output2[BATCH_SIZE:,:]\n",
        "            output_t1 = F.softmax(output_t1)\n",
        "            output_t2 = F.softmax(output_t2)\n",
        "\n",
        "            entropy_loss = - torch.mean(torch.log(torch.mean(output_t1,0)+1e-6))\n",
        "            entropy_loss -= torch.mean(torch.log(torch.mean(output_t2,0)+1e-6))\n",
        "\n",
        "            loss1 = criterion(output_s1, target1)\n",
        "            loss2 = criterion(output_s2, target1)\n",
        "            all_loss = loss1 + loss2 + 0.01 * entropy_loss\n",
        "            all_loss.backward()\n",
        "            optimizer_g.step()\n",
        "            optimizer_f.step()\n",
        "\n",
        "            #Step B train classifier to maximize discrepancy\n",
        "            optimizer_g.zero_grad()\n",
        "            optimizer_f.zero_grad()\n",
        "\n",
        "            output = G(data)\n",
        "            output1 = F1(output)\n",
        "            output2 = F2(output)\n",
        "            output_s1 = output1[:BATCH_SIZE,:]\n",
        "            output_s2 = output2[:BATCH_SIZE,:]\n",
        "            output_t1 = output1[BATCH_SIZE:,:]\n",
        "            output_t2 = output2[BATCH_SIZE:,:]\n",
        "            output_t1 = F.softmax(output_t1)\n",
        "            output_t2 = F.softmax(output_t2)\n",
        "            loss1 = criterion(output_s1, target1)\n",
        "            loss2 = criterion(output_s2, target1)\n",
        "            entropy_loss = - torch.mean(torch.log(torch.mean(output_t1,0)+1e-6))\n",
        "            entropy_loss -= torch.mean(torch.log(torch.mean(output_t2,0)+1e-6))\n",
        "            loss_dis = torch.mean(torch.abs(output_t1-output_t2))\n",
        "            F_loss = loss1 + loss2 - eta*loss_dis  + 0.01 * entropy_loss\n",
        "            F_loss.backward()\n",
        "            optimizer_f.step()\n",
        "            # Step C train genrator to minimize discrepancy\n",
        "            for i in range(N_K):\n",
        "                optimizer_g.zero_grad()\n",
        "                output = G(data)\n",
        "                output1 = F1(output)\n",
        "                output2 = F2(output)\n",
        "\n",
        "                output_s1 = output1[:BATCH_SIZE,:]\n",
        "                output_s2 = output2[:BATCH_SIZE,:]\n",
        "                output_t1 = output1[BATCH_SIZE:,:]\n",
        "                output_t2 = output2[BATCH_SIZE:,:]\n",
        "\n",
        "                loss1 = criterion(output_s1, target1)\n",
        "                loss2 = criterion(output_s2, target1)\n",
        "                output_t1 = F.softmax(output_t1)\n",
        "                output_t2 = F.softmax(output_t2)\n",
        "                loss_dis = torch.mean(torch.abs(output_t1-output_t2))\n",
        "                entropy_loss = -torch.mean(torch.log(torch.mean(output_t1,0)+1e-6))\n",
        "                entropy_loss -= torch.mean(torch.log(torch.mean(output_t2,0)+1e-6))\n",
        "\n",
        "                loss_dis.backward()\n",
        "                optimizer_g.step()\n",
        "            if batch_idx % LOG_INTERVAL == 0:\n",
        "                print('Train Ep: {} [{}/{} ({:.0f}%)]\\tLoss1: {:.6f}\\tLoss2: {:.6f}\\t Dis: {:.6f} Entropy: {:.6f}'.format(\n",
        "                    ep, batch_idx * len(data), 70000,\n",
        "                    100. * batch_idx / 70000, loss1.data[0],loss2.data[0],loss_dis.data[0],entropy_loss.data[0]))\n",
        "            if batch_idx == 1 and ep > 1:\n",
        "                test(ep)\n",
        "                G.train()\n",
        "                F1.train()\n",
        "                F2.train()"
      ],
      "metadata": {
        "id": "e1i6puNCj-mb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(epoch):\n",
        "    G.eval()\n",
        "    F1.eval()\n",
        "    F2.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    correct2 = 0\n",
        "    size = 0\n",
        "\n",
        "    for batch_idx, data in enumerate(dataset_test):\n",
        "        if batch_idx*BATCH_SIZE > 5000:\n",
        "            break\n",
        "        if cuda:\n",
        "            data2  = data['T']\n",
        "            target2 = data['T_label']\n",
        "            # if val:\n",
        "            #     data2  = data['S']\n",
        "            #     target2 = data['S_label']\n",
        "            data2, target2 = data2.cuda(), target2.cuda()\n",
        "        data1, target1 = Variable(data2, volatile=True), Variable(target2)\n",
        "        output = G(data1)\n",
        "        output1 = F1(output)\n",
        "        output2 = F2(output)\n",
        "        test_loss += F.nll_loss(output1, target1).data[0]\n",
        "        pred = output1.data.max(1)[1] # get the index of the max log-probability\n",
        "        correct += pred.eq(target1.data).cpu().sum()\n",
        "        pred = output2.data.max(1)[1] # get the index of the max log-probability\n",
        "        k = target1.data.size()[0]\n",
        "        correct2 += pred.eq(target1.data).cpu().sum()\n",
        "\n",
        "        size += k\n",
        "    test_loss = test_loss\n",
        "    test_loss /= len(test_loader) # loss function already averages over batch size\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%) ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, size,\n",
        "        100. * correct / size,100.*correct2/size))\n",
        "    #if 100. * correct / size > 67 or 100. * correct2 / size > 67:\n",
        "    value = max(100. * correct / size,100. * correct2 / size)\n",
        "    if value > 60: # TODO need to save the csv\n",
        "        save_path_pref = os.path.join(output_root, model_name+'_'+str(value)+'_')\n",
        "        torch.save(F1.state_dict(), save_path_pref+'F1.pth')\n",
        "        torch.save(F2.state_dict(), save_path_pref+'F2.pth')\n",
        "        torch.save(G.state_dict(), save_path_pref+'G.pth')"
      ],
      "metadata": {
        "id": "0UfmaLrhkBwa"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(N_EPOCH+1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DFVxchTLkEK7",
        "outputId": "b4e8e3ae-b25c-44f4-8bb4-918d382e4c94"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-b3767fa04d85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_EPOCH\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-190f437617bb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_epoch)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0moptimizer_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0moutput1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0moutput2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-9672f1bf1e82>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, reverse)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_reverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlambd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x512 and 2048x1000)"
          ]
        }
      ]
    }
  ]
}