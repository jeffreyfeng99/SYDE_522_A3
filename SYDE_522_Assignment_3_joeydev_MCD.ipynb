{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SYDE-522_Assignment_3_joeydev_MCD.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO6DAywOR/Wt3UBg1FVW8uR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeffreyfeng99/SYDE_522_A3/blob/master/SYDE_522_Assignment_3_joeydev_MCD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.utils.data as data\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.autograd import Variable\n",
        "from torch.autograd import Function\n",
        "\n",
        "from PIL import Image\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "M9VQaWUFPd_D"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lz7QvnqDPjnJ",
        "outputId": "f5e583ed-ec6d-44d6-fe44-112873afb72b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cuda = True\n",
        "cudnn.benchmark = True\n",
        "\n",
        "model_name = \"resnet18\"\n",
        "optim_name = \"momentum\"\n",
        "\n",
        "LR = 1e-3\n",
        "MOMENTUM = 0.9\n",
        "BATCH_SIZE = 32\n",
        "IMAGE_SIZE = 224 #227\n",
        "# FT_OUT_SIZE = 512\n",
        "N_EPOCH = 50\n",
        "\n",
        "AUGMENT = False\n",
        "N_K = 4 # how many steps to repeat the generator update?\n",
        "N_LAYER = 2 # how many layers for classifier\n",
        "LOG_INTERVAL = 50\n"
      ],
      "metadata": {
        "id": "cm9pIM1sPcNc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0-jDAJQ1PWNc"
      },
      "outputs": [],
      "source": [
        "dataset_root = '/content/drive/MyDrive/4B/SYDE-522/data'\n",
        "output_root = '/content/drive/MyDrive/4B/SYDE-522/submission/03302022_'+model_name\n",
        "source_dataset_name = 'train_set'\n",
        "target_dataset_name = 'test_set'\n",
        "\n",
        "source_image_root = os.path.join(dataset_root, source_dataset_name)\n",
        "target_image_root = os.path.join(dataset_root, target_dataset_name)\n",
        "\n",
        "train_path = os.path.join(source_image_root, 'train_set')\n",
        "val_path = os.path.join(target_image_root, 'test_set')\n",
        "\n",
        "train_label_list = os.path.join(dataset_root, 'train_labels.csv')\n",
        "\n",
        "os.makedirs(output_root, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GradReverse(Function):\n",
        "    def __init__(self, lambd):\n",
        "        self.lambd = lambd\n",
        "    def forward(self, x):\n",
        "        return x.view_as(x)\n",
        "    def backward(self, grad_output):\n",
        "        return (grad_output*-self.lambd)\n",
        "\n",
        "def grad_reverse(x,lambd=1.0):\n",
        "    return GradReverse(lambd)(x)\n",
        "\n",
        "class ResBase(nn.Module):\n",
        "    def __init__(self,option='resnet18',pret=True):\n",
        "        super(ResBase, self).__init__()\n",
        "        self.dim = 2048\n",
        "        if option == 'resnet18':\n",
        "            model_ft = models.resnet18(pretrained=pret)\n",
        "            self.dim = 512\n",
        "        if option == 'resnet50':\n",
        "            model_ft = models.resnet50(pretrained=pret)\n",
        "        if option == 'resnet101':\n",
        "            model_ft = models.resnet101(pretrained=pret)\n",
        "        if option == 'resnet152':\n",
        "            model_ft = models.resnet152(pretrained=pret)\n",
        "        mod = list(model_ft.children())\n",
        "        mod.pop()\n",
        "        #self.model_ft =model_ft\n",
        "        self.features = nn.Sequential(*mod)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), self.dim)\n",
        "        return x\n",
        "\n",
        "class ResClassifier(nn.Module):\n",
        "    def __init__(self, num_classes=7, num_layer=2, num_unit=2048, prob=0.5, middle=1000):\n",
        "        super(ResClassifier, self).__init__()\n",
        "        layers = []\n",
        "        # currently 10000 units\n",
        "        layers.append(nn.Dropout(p=prob))\n",
        "        layers.append(nn.Linear(num_unit,middle))\n",
        "        layers.append(nn.BatchNorm1d(middle,affine=True))\n",
        "        layers.append(nn.ReLU(inplace=True))\n",
        "\n",
        "        for i in range(num_layer-1):\n",
        "            layers.append(nn.Dropout(p=prob))\n",
        "            layers.append(nn.Linear(middle,middle))\n",
        "            layers.append(nn.BatchNorm1d(middle,affine=True))\n",
        "            layers.append(nn.ReLU(inplace=True))\n",
        "        layers.append(nn.Linear(middle,num_classes))\n",
        "        self.classifier = nn.Sequential(*layers)\n",
        "\n",
        "        #self.classifier = nn.Sequential(\n",
        "        #    nn.Dropout(),\n",
        "        #    nn.Linear(2048, 1000),\n",
        "        #    nn.BatchNorm1d(1000,affine=True),\n",
        "        #    nn.ReLU(inplace=True),\n",
        "        #    nn.Dropout(),\n",
        "        #    nn.Linear(1000, 1000),\n",
        "        #    nn.BatchNorm1d(1000,affine=True),\n",
        "        #    nn.ReLU(inplace=True),\n",
        "        #    nn.Linear(1000, num_classes),\n",
        "\n",
        "    def set_lambda(self, lambd):\n",
        "        self.lambd = lambd\n",
        "    def forward(self, x,reverse=False):\n",
        "        if reverse:\n",
        "            x = grad_reverse(x, self.lambd)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "hPPa_PHQQbJb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GetLoader(data.Dataset):\n",
        "    def __init__(self, data_root, data_list=None, transform=None):\n",
        "        self.root = data_root\n",
        "        self.transform = transform\n",
        "\n",
        "        # we only pass data_list if it's training set\n",
        "        if data_list is not None:\n",
        "            df = pd.read_csv(data_list)\n",
        "            self.img_paths = df['dir'].to_list()\n",
        "\n",
        "            if 'label2' in df.columns:\n",
        "                self.img_labels = df['label2'].to_list()\n",
        "            else: \n",
        "                self.img_labels = ['0' for i in range(len(self.img_paths))]\n",
        "\n",
        "            if 'label1' in df.columns:\n",
        "                self.domain_labels = df['label1'].to_list()\n",
        "            else: \n",
        "                self.domain_labels = ['0' for i in range(len(self.img_paths))]\n",
        "        else:\n",
        "            # Walk through test folder - we don't need labels\n",
        "            self.img_paths = [f for root,dirs,files in os.walk(data_root) for f in files if f.endswith('.png')]\n",
        "            self.img_labels = ['0' for i in range(len(self.img_paths))]\n",
        "            self.domain_labels = ['0' for i in range(len(self.img_paths))]\n",
        "\n",
        "        self.n_data = len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        img_paths, labels, domain_labels = self.img_paths[item%self.n_data], self.img_labels[item%self.n_data], self.domain_labels[item%self.n_data]\n",
        "        imgs = Image.open(os.path.join(self.root, img_paths)).convert('RGB')\n",
        "\n",
        "        if self.transform is not None:\n",
        "\n",
        "            if isinstance(self.transform, list):\n",
        "                tform = self.transform[int(domain_labels)]\n",
        "            else:\n",
        "                tform = self.transform\n",
        "\n",
        "            imgs = tform(imgs)\n",
        "            labels = int(labels)\n",
        "            domain_labels = int(domain_labels)\n",
        "\n",
        "        return imgs, labels #, domain_labels, img_paths\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_data\n",
        "\n",
        "class PairedData(object):\n",
        "    def __init__(self, data_loader_A, data_loader_B, max_dataset_size, flip):\n",
        "        self.data_loader_A = data_loader_A\n",
        "        self.data_loader_B = data_loader_B\n",
        "        self.stop_A = False\n",
        "        self.stop_B = False\n",
        "        self.max_dataset_size = max_dataset_size\n",
        "        self.flip = flip\n",
        "\n",
        "    def __iter__(self):\n",
        "        self.stop_A = False\n",
        "        self.stop_B = False\n",
        "        self.data_loader_A_iter = iter(self.data_loader_A)\n",
        "        self.data_loader_B_iter = iter(self.data_loader_B)\n",
        "        self.iter = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        A, A_paths = None, None\n",
        "        B, B_paths = None, None\n",
        "        try:\n",
        "            A, A_paths = next(self.data_loader_A_iter)\n",
        "        except StopIteration:\n",
        "            if A is None or A_paths is None:\n",
        "                self.stop_A = True\n",
        "                self.data_loader_A_iter = iter(self.data_loader_A)\n",
        "                A, A_paths = next(self.data_loader_A_iter)\n",
        "\n",
        "        try:\n",
        "            B, B_paths = next(self.data_loader_B_iter)\n",
        "        except StopIteration:\n",
        "            if B is None or B_paths is None:\n",
        "                self.stop_B = True\n",
        "                self.data_loader_B_iter = iter(self.data_loader_B)\n",
        "                B, B_paths = next(self.data_loader_B_iter)\n",
        "\n",
        "        if (self.stop_A and self.stop_B) or self.iter > self.max_dataset_size:\n",
        "            self.stop_A = False\n",
        "            self.stop_B = False\n",
        "            raise StopIteration()\n",
        "        else:\n",
        "            self.iter += 1\n",
        "            if self.flip and random.random() < 0.5:\n",
        "                idx = [i for i in range(A.size(3) - 1, -1, -1)]\n",
        "                idx = torch.LongTensor(idx)\n",
        "                A = A.index_select(3, idx)\n",
        "                B = B.index_select(3, idx)\n",
        "            return {'S': A, 'S_label': A_paths, # TODO: prob not exactly what we want\n",
        "                    'T': B, 'T_label': B_paths}\n",
        "\n",
        "class CVDataLoader(object):\n",
        "    def initialize(self, dataset_A,dataset_B,batch_size,shuffle=True):\n",
        "        #normalize = transforms.Normalize(mean=mean_im,std=std_im)\n",
        "        self.max_dataset_size = float(\"inf\")\n",
        "        data_loader_A = torch.utils.data.DataLoader(\n",
        "            dataset_A,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=shuffle,\n",
        "            num_workers=4,\n",
        "            drop_last=True)\n",
        "        data_loader_B = torch.utils.data.DataLoader(\n",
        "            dataset_B,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=shuffle,\n",
        "            num_workers=4,\n",
        "            drop_last=True)\n",
        "        self.dataset_A = dataset_A\n",
        "        self.dataset_B = dataset_B\n",
        "        flip = False\n",
        "        self.paired_data = PairedData(data_loader_A, data_loader_B, self.max_dataset_size, flip)\n",
        "\n",
        "    def name(self):\n",
        "        return 'UnalignedDataLoader'\n",
        "\n",
        "    def load_data(self):\n",
        "        return self.paired_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return max([len(self.dataset_A), len(self.dataset_B)])\n"
      ],
      "metadata": {
        "id": "VdXwNYiojYOV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_fn(mu=(0.6399, 0.6076, 0.5603), std=(0.3065, 0.3082, 0.3353), augment=False):\n",
        "    if augment:\n",
        "        # I guess we will also try augmentation\n",
        "        img_transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.CenterCrop(IMAGE_SIZE),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=mu, std=std)\n",
        "        ])\n",
        "    else:\n",
        "        img_transform = transforms.Compose([\n",
        "          transforms.Resize(IMAGE_SIZE),\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize(mean=mu, std=std) \n",
        "        ])\n",
        "\n",
        "    return img_transform\n",
        "\n",
        "def prep_dataloader(trds, vlds, shuffle=False):\n",
        "    dataloader = CVDataLoader()\n",
        "    dataloader.initialize(trds, vlds, BATCH_SIZE, shuffle=shuffle)\n",
        "    dataset = dataloader.load_data()\n",
        "    \n",
        "    return dataloader, dataset"
      ],
      "metadata": {
        "id": "oWEg2-CVPbM5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_transforms = {\n",
        "    train_path: preprocess_fn(augment=True),\n",
        "    val_path: preprocess_fn(augment=True)\n",
        "}\n",
        "\n",
        "\n",
        "datasets = {\n",
        "        train_path: GetLoader(\n",
        "                data_root=train_path,\n",
        "                data_list=train_label_list,\n",
        "                transform=data_transforms[train_path]),\n",
        "        val_path: GetLoader(\n",
        "            data_root=val_path,\n",
        "            transform=data_transforms[val_path])\n",
        "        }\n",
        "\n",
        "train_loader, dataset = prep_dataloader(datasets[train_path], datasets[val_path])\n",
        "test_loader, dataset_test = prep_dataloader(datasets[train_path], datasets[val_path], shuffle=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xc5-FXNXPs4V",
        "outputId": "9116a8c1-3371-41b4-f754-c5f2cbc7879d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data_loader_A = torch.utils.data.DataLoader(\n",
        "#             datasets[train_path],\n",
        "#             batch_size=BATCH_SIZE,\n",
        "#             shuffle=False,\n",
        "#             num_workers=4)\n",
        "# data_loader_A_iter = iter(data_loader_A)"
      ],
      "metadata": {
        "id": "JC-1Q-muWOPf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_gpu = torch.cuda.is_available()\n",
        "manual_seed = random.randint(1, 10000)\n",
        "random.seed(manual_seed)\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(manual_seed)\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.01)\n",
        "        m.bias.data.normal_(0.0, 0.01)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.01)\n",
        "        m.bias.data.fill_(0)\n",
        "    elif classname.find('Linear') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.01)\n",
        "        m.bias.data.normal_(0.0, 0.01)\n",
        "        \n",
        "\n",
        "G = ResBase(model_name)\n",
        "F1 = ResClassifier(num_layer=N_LAYER, num_unit=512)\n",
        "F2 = ResClassifier(num_layer=N_LAYER, num_unit=512)\n",
        "F1.apply(weights_init)\n",
        "F2.apply(weights_init)\n",
        "\n",
        "if cuda:\n",
        "    G.cuda()\n",
        "    F1.cuda()\n",
        "    F2.cuda()\n",
        "if optim_name == 'momentum':\n",
        "    optimizer_g = optim.SGD(list(G.features.parameters()), lr=LR, weight_decay=0.0005)\n",
        "    optimizer_f = optim.SGD(list(F1.parameters())+list(F2.parameters()), \n",
        "                            momentum=MOMENTUM, lr=LR, weight_decay=0.0005)\n",
        "elif optim_name == 'adam':\n",
        "    optimizer_g = optim.Adam(G.features.parameters(), lr=LR,weight_decay=0.0005)\n",
        "    optimizer_f = optim.Adam(list(F1.parameters())+list(F2.parameters()), \n",
        "                             lr=LR, weight_decay=0.0005)\n",
        "else:\n",
        "    optimizer_g = optim.Adadelta(G.features.parameters(), lr=LR, weight_decay=0.0005)\n",
        "    optimizer_f = optim.Adadelta(list(F1.parameters())+list(F2.parameters()), \n",
        "                                 lr=LR, weight_decay=0.0005)   "
      ],
      "metadata": {
        "id": "TvJBuiLfjzJW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(num_epoch):\n",
        "    criterion = nn.CrossEntropyLoss().cuda()\n",
        "    for ep in range(num_epoch):\n",
        "        G.train()\n",
        "        F1.train()\n",
        "        F2.train()\n",
        "        len_dataloader = max(len(train_loader), len(test_loader))\n",
        "\n",
        "        # i = 0\n",
        "        # while i < len_dataloader:\n",
        "        for batch_idx, data in enumerate(dataset):\n",
        "            # if batch_idx * BATCH_SIZE > len_dataloader:\n",
        "            #     break\n",
        "            # data = next(dataset)\n",
        "            if cuda:\n",
        "                data1 = data['S']\n",
        "                target1 = data['S_label']\n",
        "                data2  = data['T']\n",
        "                target2 = data['T_label']\n",
        "                data1, target1 = data1.cuda(), target1.cuda()\n",
        "                data2, target2 = data2.cuda(), target2.cuda()\n",
        "            # when pretraining network source only\n",
        "            eta = 1.0\n",
        "            data = Variable(torch.cat((data1,data2),0))\n",
        "            target1 = Variable(target1)\n",
        "            # Step A train all networks to minimize loss on source\n",
        "            optimizer_g.zero_grad()\n",
        "            optimizer_f.zero_grad()\n",
        "            output = G(data)\n",
        "            output1 = F1(output)\n",
        "            output2 = F2(output)\n",
        "\n",
        "            output_s1 = output1[:BATCH_SIZE,:]\n",
        "            output_s2 = output2[:BATCH_SIZE,:]\n",
        "            output_t1 = output1[BATCH_SIZE:,:]\n",
        "            output_t2 = output2[BATCH_SIZE:,:]\n",
        "            output_t1 = F.softmax(output_t1)\n",
        "            output_t2 = F.softmax(output_t2)\n",
        "\n",
        "            entropy_loss = - torch.mean(torch.log(torch.mean(output_t1,0)+1e-6))\n",
        "            entropy_loss -= torch.mean(torch.log(torch.mean(output_t2,0)+1e-6))\n",
        "\n",
        "            loss1 = criterion(output_s1, target1)\n",
        "            loss2 = criterion(output_s2, target1)\n",
        "            all_loss = loss1 + loss2 + 0.01 * entropy_loss\n",
        "            all_loss.backward()\n",
        "            optimizer_g.step()\n",
        "            optimizer_f.step()\n",
        "\n",
        "            #Step B train classifier to maximize discrepancy\n",
        "            optimizer_g.zero_grad()\n",
        "            optimizer_f.zero_grad()\n",
        "\n",
        "            output = G(data)\n",
        "            output1 = F1(output)\n",
        "            output2 = F2(output)\n",
        "            output_s1 = output1[:BATCH_SIZE,:]\n",
        "            output_s2 = output2[:BATCH_SIZE,:]\n",
        "            output_t1 = output1[BATCH_SIZE:,:]\n",
        "            output_t2 = output2[BATCH_SIZE:,:]\n",
        "            output_t1 = F.softmax(output_t1)\n",
        "            output_t2 = F.softmax(output_t2)\n",
        "            loss1 = criterion(output_s1, target1)\n",
        "            loss2 = criterion(output_s2, target1)\n",
        "            entropy_loss = - torch.mean(torch.log(torch.mean(output_t1,0)+1e-6))\n",
        "            entropy_loss -= torch.mean(torch.log(torch.mean(output_t2,0)+1e-6))\n",
        "            loss_dis = torch.mean(torch.abs(output_t1-output_t2))\n",
        "            F_loss = loss1 + loss2 - eta*loss_dis  + 0.01 * entropy_loss\n",
        "            F_loss.backward()\n",
        "            optimizer_f.step()\n",
        "            # Step C train genrator to minimize discrepancy\n",
        "            for i in range(N_K):\n",
        "                optimizer_g.zero_grad()\n",
        "                output = G(data)\n",
        "                output1 = F1(output)\n",
        "                output2 = F2(output)\n",
        "\n",
        "                output_s1 = output1[:BATCH_SIZE,:]\n",
        "                output_s2 = output2[:BATCH_SIZE,:]\n",
        "                output_t1 = output1[BATCH_SIZE:,:]\n",
        "                output_t2 = output2[BATCH_SIZE:,:]\n",
        "\n",
        "                loss1 = criterion(output_s1, target1)\n",
        "                loss2 = criterion(output_s2, target1)\n",
        "                output_t1 = F.softmax(output_t1)\n",
        "                output_t2 = F.softmax(output_t2)\n",
        "                loss_dis = torch.mean(torch.abs(output_t1-output_t2))\n",
        "                entropy_loss = -torch.mean(torch.log(torch.mean(output_t1,0)+1e-6))\n",
        "                entropy_loss -= torch.mean(torch.log(torch.mean(output_t2,0)+1e-6))\n",
        "\n",
        "                loss_dis.backward()\n",
        "                optimizer_g.step()\n",
        "            if batch_idx % LOG_INTERVAL == 0:\n",
        "                # print(ep)\n",
        "                # print(loss1.data)\n",
        "                # print(loss2.data)\n",
        "                # print(loss_dis.data)\n",
        "                # print(entropy_loss.data)\n",
        "                print('Train Ep: {} [{}/{}]\\tLoss1: {:.6f}\\tLoss2: {:.6f}\\t Dis: {:.6f} Entropy: {:.6f}'.format(\n",
        "                    ep, batch_idx*BATCH_SIZE, len_dataloader,\n",
        "                    loss1.cpu().data.numpy(),loss2.cpu().data.numpy(),\n",
        "                    loss_dis.cpu().data.numpy(),entropy_loss.cpu().data.numpy()))\n",
        "            if batch_idx == 1 and ep > 1:\n",
        "                test(ep)\n",
        "                G.train()\n",
        "                F1.train()\n",
        "                F2.train()"
      ],
      "metadata": {
        "id": "e1i6puNCj-mb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(epoch):\n",
        "    G.eval()\n",
        "    F1.eval()\n",
        "    F2.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    correct2 = 0\n",
        "    size = 0\n",
        "\n",
        "    for batch_idx, data in enumerate(dataset_test):\n",
        "        # if batch_idx*BATCH_SIZE > 5000:\n",
        "        #     break\n",
        "        if cuda:\n",
        "            data2  = data['T']\n",
        "            target2 = data['T_label']\n",
        "            # if val:\n",
        "            #     data2  = data['S']\n",
        "            #     target2 = data['S_label']\n",
        "            data2, target2 = data2.cuda(), target2.cuda()\n",
        "        data1, target1 = Variable(data2, volatile=True), Variable(target2)\n",
        "        output = G(data1)\n",
        "        output1 = F1(output)\n",
        "        output2 = F2(output)\n",
        "        test_loss += F.nll_loss(output1, target1).cpu().data.numpy()\n",
        "        pred = output1.data.max(1)[1] # get the index of the max log-probability\n",
        "        correct += pred.eq(target1.data).cpu().sum()\n",
        "        pred = output2.data.max(1)[1] # get the index of the max log-probability\n",
        "        k = target1.data.size()[0]\n",
        "        correct2 += pred.eq(target1.data).cpu().sum()\n",
        "\n",
        "        size += k\n",
        "    test_loss = test_loss\n",
        "    test_loss /= len(test_loader) # loss function already averages over batch size\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%) ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, size,\n",
        "        100. * correct / size,100.*correct2/size))\n",
        "    #if 100. * correct / size > 67 or 100. * correct2 / size > 67:\n",
        "    value = max(100. * correct / size,100. * correct2 / size)\n",
        "    if value > 60: # TODO need to save the csv\n",
        "        save_path_pref = os.path.join(output_root, model_name+'_'+str(value)+'_')\n",
        "        torch.save(F1.state_dict(), save_path_pref+'F1.pth')\n",
        "        torch.save(F2.state_dict(), save_path_pref+'F2.pth')\n",
        "        torch.save(G.state_dict(), save_path_pref+'G.pth')"
      ],
      "metadata": {
        "id": "0UfmaLrhkBwa"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(N_EPOCH+1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFVxchTLkEK7",
        "outputId": "55107f28-fc2d-4b96-945f-1b1d2280a02e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:61: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:85: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:86: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Ep: 0 [0/6062]\tLoss1: 1.924909\tLoss2: 1.889425\t Dis: 0.030638 Entropy: 3.904876\n",
            "Train Ep: 0 [1600/6062]\tLoss1: 0.945768\tLoss2: 1.001526\t Dis: 0.062011 Entropy: 4.038435\n",
            "Train Ep: 0 [3200/6062]\tLoss1: 0.647412\tLoss2: 0.677999\t Dis: 0.085513 Entropy: 4.067057\n",
            "Train Ep: 0 [4800/6062]\tLoss1: 0.536464\tLoss2: 0.524799\t Dis: 0.131250 Entropy: 4.130460\n",
            "Train Ep: 1 [0/6062]\tLoss1: 0.273794\tLoss2: 0.175599\t Dis: 0.150846 Entropy: 4.258055\n",
            "Train Ep: 1 [1600/6062]\tLoss1: 0.282109\tLoss2: 0.337609\t Dis: 0.157715 Entropy: 4.336813\n",
            "Train Ep: 1 [3200/6062]\tLoss1: 0.395625\tLoss2: 0.462725\t Dis: 0.138113 Entropy: 4.386701\n",
            "Train Ep: 1 [4800/6062]\tLoss1: 0.465653\tLoss2: 0.456765\t Dis: 0.102982 Entropy: 4.151078\n",
            "Train Ep: 2 [0/6062]\tLoss1: 0.330407\tLoss2: 0.220862\t Dis: 0.124779 Entropy: 4.430922\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: -0.0208, Accuracy: 1372/6048 (23%) (4%)\n",
            "\n",
            "Train Ep: 2 [1600/6062]\tLoss1: 0.377532\tLoss2: 0.382385\t Dis: 0.141290 Entropy: 4.594434\n",
            "Train Ep: 2 [3200/6062]\tLoss1: 0.171740\tLoss2: 0.161757\t Dis: 0.106647 Entropy: 4.832178\n",
            "Train Ep: 2 [4800/6062]\tLoss1: 0.451751\tLoss2: 0.469987\t Dis: 0.112154 Entropy: 4.530136\n",
            "Train Ep: 3 [0/6062]\tLoss1: 0.118521\tLoss2: 0.079501\t Dis: 0.140467 Entropy: 4.496674\n",
            "\n",
            "Test set: Average loss: -0.0058, Accuracy: 854/6048 (14%) (5%)\n",
            "\n",
            "Train Ep: 3 [1600/6062]\tLoss1: 0.148845\tLoss2: 0.216151\t Dis: 0.132089 Entropy: 4.570918\n",
            "Train Ep: 3 [3200/6062]\tLoss1: 0.088099\tLoss2: 0.100654\t Dis: 0.085487 Entropy: 5.252058\n",
            "Train Ep: 3 [4800/6062]\tLoss1: 0.296297\tLoss2: 0.263926\t Dis: 0.079546 Entropy: 4.424348\n",
            "Train Ep: 4 [0/6062]\tLoss1: 0.041563\tLoss2: 0.122768\t Dis: 0.127576 Entropy: 4.667576\n",
            "\n",
            "Test set: Average loss: 0.0103, Accuracy: 648/6048 (11%) (5%)\n",
            "\n",
            "Train Ep: 4 [1600/6062]\tLoss1: 0.168379\tLoss2: 0.128866\t Dis: 0.108352 Entropy: 4.667634\n",
            "Train Ep: 4 [3200/6062]\tLoss1: 0.098568\tLoss2: 0.077752\t Dis: 0.069299 Entropy: 5.692998\n",
            "Train Ep: 4 [4800/6062]\tLoss1: 0.308979\tLoss2: 0.154356\t Dis: 0.072489 Entropy: 4.496173\n",
            "Train Ep: 5 [0/6062]\tLoss1: 0.108666\tLoss2: 0.056166\t Dis: 0.103347 Entropy: 4.512446\n",
            "\n",
            "Test set: Average loss: 0.0190, Accuracy: 584/6048 (10%) (6%)\n",
            "\n",
            "Train Ep: 5 [1600/6062]\tLoss1: 0.141377\tLoss2: 0.131794\t Dis: 0.096241 Entropy: 4.772881\n",
            "Train Ep: 5 [3200/6062]\tLoss1: 0.116153\tLoss2: 0.127333\t Dis: 0.051138 Entropy: 5.564244\n",
            "Train Ep: 5 [4800/6062]\tLoss1: 0.344555\tLoss2: 0.388034\t Dis: 0.034514 Entropy: 4.689546\n",
            "Train Ep: 6 [0/6062]\tLoss1: 0.048233\tLoss2: 0.051233\t Dis: 0.082144 Entropy: 4.519004\n",
            "\n",
            "Test set: Average loss: 0.0235, Accuracy: 571/6048 (9%) (6%)\n",
            "\n",
            "Train Ep: 6 [1600/6062]\tLoss1: 0.248019\tLoss2: 0.141212\t Dis: 0.061808 Entropy: 5.162345\n",
            "Train Ep: 6 [3200/6062]\tLoss1: 0.058687\tLoss2: 0.100437\t Dis: 0.038958 Entropy: 5.990528\n",
            "Train Ep: 6 [4800/6062]\tLoss1: 0.217576\tLoss2: 0.284059\t Dis: 0.042363 Entropy: 4.786266\n",
            "Train Ep: 7 [0/6062]\tLoss1: 0.030879\tLoss2: 0.015109\t Dis: 0.059220 Entropy: 4.374011\n",
            "\n",
            "Test set: Average loss: 0.0249, Accuracy: 567/6048 (9%) (7%)\n",
            "\n",
            "Train Ep: 7 [1600/6062]\tLoss1: 0.034541\tLoss2: 0.138844\t Dis: 0.051210 Entropy: 4.902661\n",
            "Train Ep: 7 [3200/6062]\tLoss1: 0.038453\tLoss2: 0.013205\t Dis: 0.031344 Entropy: 5.948127\n",
            "Train Ep: 7 [4800/6062]\tLoss1: 0.097459\tLoss2: 0.212556\t Dis: 0.036783 Entropy: 4.403311\n",
            "Train Ep: 8 [0/6062]\tLoss1: 0.057763\tLoss2: 0.009954\t Dis: 0.035150 Entropy: 4.300014\n",
            "\n",
            "Test set: Average loss: 0.0309, Accuracy: 560/6048 (9%) (7%)\n",
            "\n",
            "Train Ep: 8 [1600/6062]\tLoss1: 0.103599\tLoss2: 0.076515\t Dis: 0.054444 Entropy: 4.859441\n",
            "Train Ep: 8 [3200/6062]\tLoss1: 0.058375\tLoss2: 0.055224\t Dis: 0.023369 Entropy: 6.453709\n",
            "Train Ep: 8 [4800/6062]\tLoss1: 0.217484\tLoss2: 0.141668\t Dis: 0.025913 Entropy: 4.738355\n",
            "Train Ep: 9 [0/6062]\tLoss1: 0.003896\tLoss2: 0.008074\t Dis: 0.043677 Entropy: 4.366694\n",
            "\n",
            "Test set: Average loss: 0.0356, Accuracy: 525/6048 (9%) (7%)\n",
            "\n",
            "Train Ep: 9 [1600/6062]\tLoss1: 0.087276\tLoss2: 0.018563\t Dis: 0.055130 Entropy: 4.791011\n"
          ]
        }
      ]
    }
  ]
}