{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SYDE-552_Assignment-3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeffreyfeng99/SYDE_522_A3/blob/master/SYDE_522_Assignment_3_domainlabels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submission Notes\n",
        "Group members:\n",
        "\n",
        "Jeffrey Feng, 20704800\n",
        "\n",
        "Joey Kuang, 20726074"
      ],
      "metadata": {
        "id": "jFT6ZGYe2Z_X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import statements"
      ],
      "metadata": {
        "id": "D7wdh2jP2pah"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Zo5wLSmr0I_e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "from torch.autograd import Function\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torchvision import transforms\n",
        "from torchvision import datasets\n",
        "from torchvision import models\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpoWcqpj2tyC",
        "outputId": "d278eb08-c4ee-45fa-fe6d-ff360a7b12c6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config"
      ],
      "metadata": {
        "id": "hDu1261wRS3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cuda = True\n",
        "cudnn.benchmark = True\n",
        "LR = 1e-3\n",
        "BATCH_SIZE = 64\n",
        "IMAGE_SIZE = 224 #227\n",
        "FT_OUT_SIZE = 512\n",
        "N_EPOCH = 10 #100"
      ],
      "metadata": {
        "id": "15XJCiYRRSRV"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data loader"
      ],
      "metadata": {
        "id": "B8kP0taw0Y4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_root = '/content/drive/MyDrive/4B/SYDE-522/data'\n",
        "output_root = '/content/drive/MyDrive/4B/SYDE-522/submission'\n",
        "source_dataset_name = 'train_set'\n",
        "target_dataset_name = 'test_set'\n",
        "\n",
        "source_image_root = os.path.join(dataset_root, source_dataset_name)\n",
        "target_image_root = os.path.join(dataset_root, target_dataset_name)\n",
        "\n",
        "train_label_list = os.path.join(dataset_root, 'train_labels.csv')"
      ],
      "metadata": {
        "id": "NFVq3l7Y3Gq0"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GetLoader(data.Dataset):\n",
        "    def __init__(self, data_root, data_list=None, transform=None):\n",
        "        self.root = data_root\n",
        "        self.transform = transform\n",
        "\n",
        "        # we only pass data_list if it's training set\n",
        "        if data_list is not None:\n",
        "            df = pd.read_csv(data_list)\n",
        "            self.img_paths = df['dir'].to_list()\n",
        "\n",
        "            if 'label2' in df.columns:\n",
        "                self.img_labels = df['label2'].to_list()\n",
        "            else: \n",
        "                self.img_labels = ['0' for i in range(len(self.img_paths))]\n",
        "\n",
        "            if 'label1' in df.columns:\n",
        "                self.domain_labels = df['label1'].to_list()\n",
        "            else: \n",
        "                self.domain_labels = ['0' for i in range(len(self.img_paths))]\n",
        "        else:\n",
        "            # Walk through test folder - we don't need labels\n",
        "            self.img_paths = [f for root,dirs,files in os.walk(data_root) for f in files if f.endswith('.png')]\n",
        "            self.img_labels = ['0' for i in range(len(self.img_paths))]\n",
        "            self.domain_labels = ['0' for i in range(len(self.img_paths))]\n",
        "\n",
        "        self.n_data = len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        img_paths, labels, domain_labels = self.img_paths[item%self.n_data], self.img_labels[item%self.n_data], self.domain_labels[item%self.n_data]\n",
        "        imgs = Image.open(os.path.join(self.root, img_paths)).convert('RGB')\n",
        "\n",
        "        if self.transform is not None:\n",
        "            imgs = self.transform(imgs)\n",
        "            labels = int(labels)\n",
        "            domain_labels = int(domain_labels)\n",
        "\n",
        "        return imgs, labels, domain_labels, img_paths\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_data"
      ],
      "metadata": {
        "id": "oEYsAG4z0eSR"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess data\n",
        "def preprocess_fn(mu=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)):\n",
        "  img_transform = transforms.Compose([\n",
        "      transforms.Resize(IMAGE_SIZE),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)) # why isn't this (0.1307,) and (0.3081,) like in test?\n",
        "  ])\n",
        "\n",
        "  return img_transform\n",
        "\n",
        "def prep_dataloader(image_root, label_list=None, img_transform=None, \n",
        "                    drop_last=False, shuffle=True):\n",
        "    dataset = GetLoader(\n",
        "        data_root=image_root,\n",
        "        data_list=label_list,\n",
        "        transform=img_transform\n",
        "    )\n",
        "\n",
        "    dataloader = data.DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=8,\n",
        "        drop_last=drop_last)\n",
        "    \n",
        "    return dataset, dataloader\n"
      ],
      "metadata": {
        "id": "E_Qa3TmphXFW"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model definition"
      ],
      "metadata": {
        "id": "iIIkSqtV0mC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if False, then we are feature extracting\n",
        "def set_parameter_requires_grad(model, finetune):\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = finetune"
      ],
      "metadata": {
        "id": "v2jKLmUOUbb0"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReverseLayerF(Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, alpha):\n",
        "        ctx.alpha = alpha\n",
        "\n",
        "        return x.view_as(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        output = grad_output.neg() * ctx.alpha\n",
        "\n",
        "        return output, None\n",
        "\n",
        "\n",
        "class CNNModel(nn.Module):\n",
        "\n",
        "    def __init__(self, model_name=\"resnet18\"):\n",
        "        super(CNNModel, self).__init__()\n",
        "\n",
        "        if model_name == \"resnet18\":\n",
        "            self.feature = models.resnet18(pretrained=True) \n",
        "            self.feature.fc = nn.Identity()\n",
        "        elif model_name == \"vgg11\":\n",
        "            self.feature = models.vgg11_bn(pretrained=True) \n",
        "            self.feature.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1)) # original is (7,7)\n",
        "            self.feature.classifier = nn.Identity()\n",
        "        else:\n",
        "            # Need some default model?\n",
        "            self.feature = nn.Sequential()\n",
        "            self.feature.add_module('f_conv1', nn.Conv2d(3, 64, kernel_size=5))\n",
        "            self.feature.add_module('f_bn1', nn.BatchNorm2d(64))\n",
        "            self.feature.add_module('f_pool1', nn.MaxPool2d(2))\n",
        "            self.feature.add_module('f_relu1', nn.ReLU(True))\n",
        "            self.feature.add_module('f_conv2', nn.Conv2d(64, 128, kernel_size=3))\n",
        "            self.feature.add_module('f_bn2', nn.BatchNorm2d(128))\n",
        "            self.feature.add_module('f_drop1', nn.Dropout2d())\n",
        "            self.feature.add_module('f_pool2', nn.MaxPool2d(2))\n",
        "            self.feature.add_module('f_relu2', nn.ReLU(True))\n",
        "            self.feature.add_module('f_conv3', nn.Conv2d(128, 256, kernel_size=3))\n",
        "            self.feature.add_module('f_bn3', nn.BatchNorm2d(256))\n",
        "            self.feature.add_module('f_drop3', nn.Dropout2d())\n",
        "            self.feature.add_module('f_pool3', nn.MaxPool2d(2))\n",
        "            self.feature.add_module('f_relu4', nn.ReLU(True))\n",
        "            self.feature.add_module('f_conv4', nn.Conv2d(256, 256, kernel_size=3))\n",
        "            self.feature.add_module('f_bn4', nn.BatchNorm2d(256))\n",
        "            self.feature.add_module('f_drop4', nn.Dropout2d())\n",
        "            self.feature.add_module('f_pool4', nn.MaxPool2d(2))\n",
        "            self.feature.add_module('f_relu4', nn.ReLU(True))\n",
        "            self.feature.add_module('f_conv5', nn.Conv2d(256, 512, kernel_size=3))\n",
        "            self.feature.add_module('f_bn5', nn.BatchNorm2d(512))\n",
        "            self.feature.add_module('f_drop5', nn.Dropout2d())\n",
        "            self.feature.add_module('f_pool5', nn.MaxPool2d(2))\n",
        "            self.feature.add_module('f_relu5', nn.ReLU(True))\n",
        "\n",
        "        self.class_classifier = nn.Sequential()\n",
        "        self.class_classifier.add_module('c_fc1', nn.Linear(FT_OUT_SIZE, 100))\n",
        "        self.class_classifier.add_module('c_bn1', nn.BatchNorm1d(100))\n",
        "        self.class_classifier.add_module('c_relu1', nn.ReLU(True))\n",
        "        self.class_classifier.add_module('c_drop1', nn.Dropout2d())\n",
        "        self.class_classifier.add_module('c_fc2', nn.Linear(100, 100))\n",
        "        self.class_classifier.add_module('c_bn2', nn.BatchNorm1d(100))\n",
        "        self.class_classifier.add_module('c_relu2', nn.ReLU(True))\n",
        "        self.class_classifier.add_module('c_fc3', nn.Linear(100, 7))\n",
        "        self.class_classifier.add_module('c_softmax', nn.LogSoftmax(dim=1))\n",
        "\n",
        "        self.domain_classifier = nn.Sequential()\n",
        "        self.domain_classifier.add_module('d_fc1', nn.Linear(FT_OUT_SIZE, 100))\n",
        "        self.domain_classifier.add_module('d_bn1', nn.BatchNorm1d(100))\n",
        "        self.domain_classifier.add_module('d_relu1', nn.ReLU(True))\n",
        "        self.domain_classifier.add_module('d_fc2', nn.Linear(100, 4))\n",
        "        self.domain_classifier.add_module('d_softmax', nn.LogSoftmax(dim=1))\n",
        "\n",
        "    def forward(self, input_data, alpha):\n",
        "        input_data = input_data.expand(input_data.data.shape[0], 3, IMAGE_SIZE, IMAGE_SIZE)\n",
        "        feature = self.feature(input_data)\n",
        "        feature = feature.view(-1, FT_OUT_SIZE)\n",
        "        reverse_feature = ReverseLayerF.apply(feature, alpha)\n",
        "        class_output = self.class_classifier(feature)\n",
        "        domain_output = self.domain_classifier(reverse_feature)\n",
        "\n",
        "        return class_output, domain_output"
      ],
      "metadata": {
        "id": "LI3D9RwF0lcc"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test pipeline"
      ],
      "metadata": {
        "id": "cjc2oINK088z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(net, epoch):\n",
        "\n",
        "    # load data\n",
        "    img_transform_source = preprocess_fn(mu=(0.1307,), std=(0.3081,))\n",
        "    img_transform_target = preprocess_fn()\n",
        "\n",
        "    dataset_source, dataloader_source = prep_dataloader(\n",
        "        image_root=os.path.join(source_image_root, 'train_set'),\n",
        "        label_list=train_label_list,\n",
        "        img_transform=img_transform_source,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    dataset_target, dataloader_target = prep_dataloader(\n",
        "        image_root=os.path.join(target_image_root, 'test_set'),\n",
        "        img_transform=img_transform_target,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    net.eval()\n",
        "\n",
        "    if cuda:\n",
        "        net = net.cuda()\n",
        "\n",
        "    train_pths, train_preds = inference(net, dataloader_source, cuda=cuda, alpha=alpha)\n",
        "    train_results = pd.DataFrame({'id': train_pths, 'label': train_preds})\n",
        "    train_results_pth = os.path.join(output_root, '%s_train_epoch%s.csv' % (datetime.now().strftime(\"%m%d%Y\"), epoch))\n",
        "    train_results.to_csv(train_results_pth, index=False)\n",
        "\n",
        "    test_pths, test_preds = inference(net, dataloader_target, cuda=cuda, alpha=alpha)\n",
        "    test_results = pd.DataFrame({'id': test_pths, 'label': test_preds})\n",
        "    test_results_pth = os.path.join(output_root, '%s_test_epoch%s.csv' % (datetime.now().strftime(\"%m%d%Y\"), epoch))\n",
        "    test_results.to_csv(test_results_pth, index=False)\n",
        "\n",
        "    print('epoch: %d, accuracy of the train dataset: %f' % (epoch, compare(train_label_list, train_results_pth)))\n",
        "\n",
        "    # Secret\n",
        "    test_label_list = os.path.join(dataset_root, 'dummy_test_labels.csv')\n",
        "    print('epoch: %d, accuracy of the test dataset: %f' % (epoch, compare(test_label_list, test_results_pth)))\n",
        "\n",
        "def inference(net, dataloader, cuda=True, alpha=0.0):\n",
        "    preds = []\n",
        "    pths = []\n",
        "    for input_img, _,_, img_paths in dataloader: \n",
        "\n",
        "        if cuda:\n",
        "            input_img = input_img.cuda()\n",
        "\n",
        "        class_output, _ = net(input_data=input_img, alpha=alpha)\n",
        "        pred = class_output.data.max(1, keepdim=True)[1]\n",
        "        pths = pths + list(img_paths)\n",
        "        preds = preds + list(pred.squeeze(1).cpu().numpy())\n",
        "    return pths, preds\n",
        "\n",
        "def compare(true_labels, predicted_labels):\n",
        "    combined_df = pd.read_csv(true_labels)\n",
        "    predicted_df = pd.read_csv(predicted_labels)\n",
        "\n",
        "    combined_df['label'] = combined_df['dir'].map(predicted_df.set_index('id')['label'])\n",
        "\n",
        "    true_labels = np.array(combined_df['label2'].to_list())\n",
        "    pred_labels = np.array(combined_df['label'].to_list())\n",
        "\n",
        "    return np.sum(true_labels == pred_labels) / len(true_labels)"
      ],
      "metadata": {
        "id": "qmNvZ7O101l4"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training pipeline"
      ],
      "metadata": {
        "id": "Av_ezC7pH7o7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "manual_seed = random.randint(1, 10000)\n",
        "random.seed(manual_seed)\n",
        "torch.manual_seed(manual_seed)\n",
        "\n",
        "# load data\n",
        "img_transform_source = preprocess_fn()\n",
        "img_transform_target = preprocess_fn()\n",
        "\n",
        "dataset_source, dataloader_source = prep_dataloader(\n",
        "    image_root=os.path.join(source_image_root, 'train_set'), # TODO should we unnest\n",
        "    label_list=train_label_list,\n",
        "    img_transform=img_transform_source\n",
        ")\n",
        "\n",
        "dataset_target, dataloader_target = prep_dataloader(\n",
        "    image_root=os.path.join(target_image_root, 'test_set'),\n",
        "    img_transform=img_transform_target\n",
        ")\n",
        "\n",
        "# load model\n",
        "my_net = CNNModel()\n",
        "\n",
        "# setup optimizer\n",
        "optimizer = optim.Adam(my_net.parameters(), lr=LR)\n",
        "\n",
        "loss_class = torch.nn.NLLLoss()\n",
        "loss_domain = torch.nn.NLLLoss()\n",
        "\n",
        "if cuda:\n",
        "    my_net = my_net.cuda()\n",
        "    loss_class = loss_class.cuda()\n",
        "    loss_domain = loss_domain.cuda()\n",
        "\n",
        "set_parameter_requires_grad(my_net, True)\n",
        "\n",
        "# training\n",
        "for epoch in tqdm(range(N_EPOCH)):\n",
        "\n",
        "    len_dataloader = max(len(dataloader_source), len(dataloader_target))\n",
        "    data_source_iter = iter(dataloader_source)\n",
        "    data_target_iter = iter(dataloader_target)\n",
        "\n",
        "    i = 0\n",
        "    while i < len_dataloader:\n",
        "\n",
        "        p = float(i + epoch * len_dataloader) / N_EPOCH / len_dataloader\n",
        "        alpha = 2. / (1. + np.exp(-10 * p)) - 1\n",
        "\n",
        "        # training model using source data\n",
        "        data_source = data_source_iter.next()\n",
        "        s_img, s_label, s_domain_label, _ = data_source\n",
        "\n",
        "        my_net.zero_grad()\n",
        "        batch_size = len(s_label)\n",
        "\n",
        "        input_img = torch.FloatTensor(batch_size, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
        "        class_label = torch.LongTensor(batch_size)\n",
        "        domain_label = torch.LongTensor(batch_size)\n",
        "\n",
        "        if cuda:\n",
        "            s_img = s_img.cuda()\n",
        "            s_label = s_label.cuda()\n",
        "            s_domain_label = s_domain_label.cuda()\n",
        "            input_img = input_img.cuda()\n",
        "            class_label = class_label.cuda()\n",
        "            domain_label = domain_label.cuda()\n",
        "\n",
        "        input_img.resize_as_(s_img).copy_(s_img)\n",
        "        class_label.resize_as_(s_label).copy_(s_label)\n",
        "        domain_label.resize_as_(s_domain_label).copy_(s_domain_label)\n",
        "\n",
        "        class_output, domain_output = my_net(input_data=input_img, alpha=alpha)\n",
        "        err_s_label = loss_class(class_output, class_label)\n",
        "        err_s_domain = loss_domain(domain_output, domain_label)\n",
        "\n",
        "        # training model using target data\n",
        "        if i == len(dataloader_target):\n",
        "            data_target_iter = iter(dataloader_target)\n",
        "        data_target = data_target_iter.next()\n",
        "        t_img, _, _, _ = data_target\n",
        "\n",
        "        batch_size = len(t_img) # TODO: why?\n",
        "\n",
        "        input_img = torch.FloatTensor(batch_size, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
        "        domain_label = torch.ones(batch_size) * 3.0\n",
        "        domain_label = domain_label.long()\n",
        "\n",
        "        if cuda:\n",
        "            t_img = t_img.cuda()\n",
        "            input_img = input_img.cuda()\n",
        "            domain_label = domain_label.cuda()\n",
        "\n",
        "        input_img.resize_as_(t_img).copy_(t_img)\n",
        "\n",
        "        _, domain_output = my_net(input_data=input_img, alpha=alpha)\n",
        "        err_t_domain = loss_domain(domain_output, domain_label)\n",
        "        err = err_t_domain + err_s_domain + err_s_label\n",
        "        err.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        i += 1\n",
        "\n",
        "        print('epoch: %d, [iter: %d / all %d], err_s_label: %f, err_s_domain: %f, err_t_domain: %f' \\\n",
        "            % (epoch, i, len_dataloader, err_s_label.cpu().data.numpy(),\n",
        "                err_s_domain.cpu().data.numpy(), err_t_domain.cpu().data.numpy()))\n",
        "\n",
        "    # torch.save(my_net, 'output/model_epoch_{}.pth'.format(epoch))\n",
        "    test(my_net, epoch)\n",
        "    my_net.train()\n",
        "\n",
        "print('done')"
      ],
      "metadata": {
        "id": "DOfxmvGjCWRz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}