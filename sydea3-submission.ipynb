{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/jeffreyfeng99/SYDE_522_A3/blob/master/SYDE_522_Assignment_3_joeydev.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github"}},{"cell_type":"markdown","source":"## Submission Notes\nGroup members:\n\nJeffrey Feng, 20704800\n\nJoey Kuang, 20726074","metadata":{"id":"jFT6ZGYe2Z_X"}},{"cell_type":"markdown","source":"# Progress and Methodology\n\n### Progress\n\nThe first method we tried for this domain adaptation task followed unsupervised domain adaptation by backpropagation (DANN) [1, 2]. The feature extractor was a vanilla convolutional neural network and the domain and label classifiers were simple 2 to 3 layer dense networks. Batch normalization, ReLU, and dropout layers (P=0.5) were used throughout the network, and loss was determined by negative log likelihood. We first experimented with changing the feature extractor to pretrained state-of-the-art networks such as ResNet-34, VGG16, and EfficientNet-b0 [3, 4, 5]. We also tried larger networks such as the larger ResNet and EfficientNet variations. EfficientNet-b4 provided the best result. The two classifiers were also modified by adding layers, but this did not improve performance so we reverted for subsequent experiments. \n\nWe also experimented with various training algorithms such as adversarial discriminative domain adaptation (ADDA), maximum classifier discrepancy (MCD), and DIRT-T [6, 7, 8]. We found that none of these provided stable accuracies, nor did they provide greater training accuracy than using EfficientNet-b0 on DANN. Additionally, we explored the effects of different data processing. We first apply dataset-specific normalization. We also tried applying domain-specific normalization, but this did not significantly improve results. To make domains appear similar to each other, edge detection is applied on inputs so that contours of objects are fed into the network rather than the full context of the image. This method performed decently, but could not outperform using the data as they were. We applied data augmentation based on the VISDA-2017 visual domain adaptation challenge winner [9]. This was significantly boosting performances from all our experimental baselines. \n\n### Final Method\n\nOur setup is adapted from the DANN github repo [10]. The final method we arrived at uses the DANN algorithm to finetune an EfficientNet-b4 feature extractor pretrained on ImageNet, and two vanilla dense networks to classify the domain and label. Negative log likelihood loss is determined for each network and the sum is carried in backpropagation. We apply data augmentation that includes random crop, rotation, scale, horizontal flips, brightness scaling, and desaturation before applying dataset-wide normalization. The final input image size is 224x224 rather than the original 227x227 to accommodate the EfficientNet-b4 input size. We train with batch normalization and a batch size of 32 for 300 epochs at a learning rate of 1e-4. The Kaggle environment supported network training.\n\n### References\n\n[1] Y. Ganin and V. Lempitsky, “Unsupervised Domain Adaptation by Backpropagation,” arXiv:1409.7495 [cs, stat], Feb. 2015, Accessed: Apr. 14, 2022. [Online]. Available: http://arxiv.org/abs/1409.7495\n\n[2] Y. Ganin et al., “Domain-Adversarial Training of Neural Networks,” arXiv:1505.07818 [cs, stat], May 2016, Accessed: Apr. 14, 2022. [Online]. Available: http://arxiv.org/abs/1505.07818\n\n[3] K. He, X. Zhang, S. Ren, and J. Sun, “Deep Residual Learning for Image Recognition,” arXiv:1512.03385 [cs], Dec. 2015, Accessed: Apr. 14, 2022. [Online]. Available: http://arxiv.org/abs/1512.03385\n\n[4] K. Simonyan and A. Zisserman, “Very Deep Convolutional Networks for Large-Scale Image Recognition,” arXiv:1409.1556 [cs], Apr. 2015, Accessed: Apr. 14, 2022. [Online]. Available: http://arxiv.org/abs/1409.1556\n\n[5] M. Tan and Q. V. Le, “EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,” arXiv:1905.11946 [cs, stat], Sep. 2020, Accessed: Apr. 14, 2022. [Online]. Available: http://arxiv.org/abs/1905.11946\n\n[6] E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell, “Adversarial Discriminative Domain Adaptation,” arXiv:1702.05464 [cs], Feb. 2017, Accessed: Apr. 14, 2022. [Online]. Available: http://arxiv.org/abs/1702.05464\n\n[7] K. Saito, K. Watanabe, Y. Ushiku, and T. Harada, “Maximum Classifier Discrepancy for Unsupervised Domain Adaptation,” arXiv:1712.02560 [cs], Apr. 2018, Accessed: Apr. 14, 2022. [Online]. Available: http://arxiv.org/abs/1712.02560\n\n[8] R. Shu, H. H. Bui, H. Narui, and S. Ermon, “A DIRT-T Approach to Unsupervised Domain Adaptation,” arXiv:1802.08735 [cs, stat], Mar. 2018, Accessed: Apr. 14, 2022. [Online]. Available: http://arxiv.org/abs/1802.08735\n\n[9] G. French, M. Mackiewicz, and M. Fisher, “Self-ensembling for visual domain adaptation,” arXiv:1706.05208 [cs], Sep. 2018, Accessed: Apr. 14, 2022. [Online]. Available: http://arxiv.org/abs/1706.05208\n\n[10] “DANN/model.py at master · fungtion/DANN,” GitHub. https://github.com/fungtion/DANN (accessed Apr. 14, 2022).\n\n","metadata":{}},{"cell_type":"markdown","source":"# Import statements","metadata":{"id":"D7wdh2jP2pah"}},{"cell_type":"code","source":"# This version is required to use efficientnetsb0-b7\n!pip install torchvision==0.11.1","metadata":{"execution":{"iopub.status.busy":"2022-04-05T01:45:21.956826Z","iopub.execute_input":"2022-04-05T01:45:21.957093Z","iopub.status.idle":"2022-04-05T01:45:29.867914Z","shell.execute_reply.started":"2022-04-05T01:45:21.957065Z","shell.execute_reply":"2022-04-05T01:45:29.866942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport random\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom datetime import datetime\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.data as data\nfrom torch.autograd import Function\nimport torch.backends.cudnn as cudnn\nfrom torchvision import transforms\nfrom torchvision import datasets\nfrom torchvision import models\nimport torch.optim as optim","metadata":{"id":"Zo5wLSmr0I_e","execution":{"iopub.status.busy":"2022-04-05T01:45:29.871045Z","iopub.execute_input":"2022-04-05T01:45:29.87147Z","iopub.status.idle":"2022-04-05T01:45:29.87824Z","shell.execute_reply.started":"2022-04-05T01:45:29.871429Z","shell.execute_reply":"2022-04-05T01:45:29.877463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config","metadata":{"id":"hDu1261wRS3V"}},{"cell_type":"code","source":"cuda = True\ncudnn.benchmark = True\n\nmodel_name = \"efficientnet_b4\"\n\nLR = 1e-4\nBATCH_SIZE = 32\nIMAGE_SIZE = 224\nN_EPOCH = 200","metadata":{"id":"15XJCiYRRSRV","execution":{"iopub.status.busy":"2022-04-05T01:45:29.890692Z","iopub.execute_input":"2022-04-05T01:45:29.891044Z","iopub.status.idle":"2022-04-05T01:45:29.897697Z","shell.execute_reply.started":"2022-04-05T01:45:29.890999Z","shell.execute_reply":"2022-04-05T01:45:29.896937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data loader","metadata":{"id":"B8kP0taw0Y4e"}},{"cell_type":"code","source":"dataset_root = '../input/syde522a3data/data' # path to input data uploaded to kaggle directory\noutput_root = './04092022_efficientnetb4_aug_run1'# desired output pathname that will be saved under Output /kaggle/working\nsource_dataset_name = 'train_set'\ntarget_dataset_name = 'test_set'\n\nsource_image_root = os.path.join(dataset_root, source_dataset_name)\ntarget_image_root = os.path.join(dataset_root, target_dataset_name)\n\ntrain_label_list = os.path.join(dataset_root, 'train_labels.csv')\n\nos.makedirs(output_root, exist_ok=True)","metadata":{"id":"NFVq3l7Y3Gq0","execution":{"iopub.status.busy":"2022-04-05T01:45:29.898498Z","iopub.execute_input":"2022-04-05T01:45:29.898772Z","iopub.status.idle":"2022-04-05T01:45:29.909226Z","shell.execute_reply.started":"2022-04-05T01:45:29.898738Z","shell.execute_reply":"2022-04-05T01:45:29.908448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GetLoader(data.Dataset):\n    def __init__(self, data_root, data_list=None, transform=None):\n        self.root = data_root\n        self.transform = transform\n\n        # we only pass data_list if it's training set\n        if data_list is not None:\n            df = pd.read_csv(data_list)\n            self.img_paths = df['dir'].to_list()\n\n            if 'label2' in df.columns:\n                self.img_labels = df['label2'].to_list()\n            else: \n                self.img_labels = ['0' for i in range(len(self.img_paths))]\n\n            if 'label1' in df.columns:\n                self.domain_labels = df['label1'].to_list()\n            else: \n                self.domain_labels = ['0' for i in range(len(self.img_paths))]\n        else:\n            # Walk through test folder - we don't need labels\n            self.img_paths = [f for root,dirs,files in os.walk(data_root) for f in files if f.endswith('.png')]\n            self.img_labels = ['0' for i in range(len(self.img_paths))]\n            self.domain_labels = ['0' for i in range(len(self.img_paths))]\n\n        self.n_data = len(self.img_paths)\n\n    def __getitem__(self, item):\n        img_paths, labels, domain_labels = self.img_paths[item%self.n_data], self.img_labels[item%self.n_data], self.domain_labels[item%self.n_data]\n        imgs = Image.open(os.path.join(self.root, img_paths)).convert('RGB')\n\n        if self.transform is not None:\n\n            if isinstance(self.transform, list):\n                tform = self.transform[int(domain_labels)]\n            else:\n                tform = self.transform\n\n            imgs = tform(imgs)\n            labels = int(labels)\n            domain_labels = int(domain_labels)\n\n        return imgs, labels, domain_labels, img_paths\n\n    def __len__(self):\n        return self.n_data","metadata":{"id":"oEYsAG4z0eSR","execution":{"iopub.status.busy":"2022-04-05T01:45:29.910371Z","iopub.execute_input":"2022-04-05T01:45:29.911229Z","iopub.status.idle":"2022-04-05T01:45:29.927207Z","shell.execute_reply.started":"2022-04-05T01:45:29.911193Z","shell.execute_reply":"2022-04-05T01:45:29.926355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocess data\ndef preprocess_multiple_fn(mus, stds):\n    tforms = []\n\n    for i in range(len(mus)):\n        tforms.append(preprocess_fn(mu=mus[i], std=stds[i]))\n    \n    return tforms\n\ndef preprocess_fn(mu=(0.6399, 0.6076, 0.5603), std=(0.3065, 0.3082, 0.3353), aug=False):\n  if aug:\n    img_transform = transforms.Compose([\n        transforms.RandomCrop(IMAGE_SIZE),\n        transforms.RandomHorizontalFlip(0.5),\n        transforms.RandomRotation(15),\n        transforms.ColorJitter(brightness=0.5, saturation=0.5),\n        transforms.Resize(IMAGE_SIZE),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=mu, std=std) \n    ])\n  else:\n    img_transform = transforms.Compose([\n        transforms.Resize(IMAGE_SIZE),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=mu, std=std) \n    ])\n\n  return img_transform\n\ndef prep_dataloader(image_root, label_list=None, img_transform=None, \n                    drop_last=False, shuffle=True):\n    dataset = GetLoader(\n        data_root=image_root,\n        data_list=label_list,\n        transform=img_transform\n    )\n\n    dataloader = data.DataLoader(\n        dataset=dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=shuffle,\n        num_workers=4,\n        drop_last=drop_last)\n    \n    return dataset, dataloader\n","metadata":{"id":"E_Qa3TmphXFW","execution":{"iopub.status.busy":"2022-04-05T01:45:29.928773Z","iopub.execute_input":"2022-04-05T01:45:29.929359Z","iopub.status.idle":"2022-04-05T01:45:29.941194Z","shell.execute_reply.started":"2022-04-05T01:45:29.929307Z","shell.execute_reply":"2022-04-05T01:45:29.940346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model definition","metadata":{"id":"iIIkSqtV0mC9"}},{"cell_type":"code","source":"# if False, then we are feature extracting\ndef set_parameter_requires_grad(model, finetune):\n    for param in model.parameters():\n        param.requires_grad = finetune","metadata":{"id":"v2jKLmUOUbb0","execution":{"iopub.status.busy":"2022-04-05T01:45:29.942803Z","iopub.execute_input":"2022-04-05T01:45:29.943431Z","iopub.status.idle":"2022-04-05T01:45:29.954076Z","shell.execute_reply.started":"2022-04-05T01:45:29.943379Z","shell.execute_reply":"2022-04-05T01:45:29.953275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ReverseLayerF(Function):\n\n    @staticmethod\n    def forward(ctx, x, alpha):\n        ctx.alpha = alpha\n\n        return x.view_as(x)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        output = grad_output.neg() * ctx.alpha\n\n        return output, None\n\n\nclass CNNModel(nn.Module):\n\n    def __init__(self, model_name=\"resnet18\"):\n        super(CNNModel, self).__init__()\n        self.ft_out_size = 0\n\n        if model_name == \"resnet18\":\n            self.feature = models.resnet18(pretrained=True) \n            self.feature.fc = nn.Identity()\n            self.ft_out_size = 512\n        if model_name == \"resnet50\":\n            self.feature = models.resnet50(pretrained=True) \n            self.feature.fc = nn.Identity()\n            self.ft_out_size = 2048\n        elif model_name == \"vgg16\":\n            self.feature = models.vgg16_bn(pretrained=True) \n            self.feature.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1)) # original is (7,7)\n            self.feature.classifier = nn.Identity()\n            self.ft_out_size = 512\n        elif model_name == \"efficientnet_b4\":\n            self.feature = models.efficientnet_b4(pretrained=True) \n            self.feature.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1)) # original is (7,7)\n            self.feature.classifier = nn.Identity()\n            self.ft_out_size = 1792\n        else:\n            # Default feature extracting model\n            self.feature = nn.Sequential()\n            self.feature.add_module('f_conv1', nn.Conv2d(3, 64, kernel_size=5))\n            self.feature.add_module('f_bn1', nn.BatchNorm2d(64))\n            self.feature.add_module('f_pool1', nn.MaxPool2d(2))\n            self.feature.add_module('f_relu1', nn.ReLU(True))\n            self.feature.add_module('f_conv2', nn.Conv2d(64, 128, kernel_size=3))\n            self.feature.add_module('f_bn2', nn.BatchNorm2d(128))\n            self.feature.add_module('f_drop1', nn.Dropout2d())\n            self.feature.add_module('f_pool2', nn.MaxPool2d(2))\n            self.feature.add_module('f_relu2', nn.ReLU(True))\n            self.feature.add_module('f_conv3', nn.Conv2d(128, 256, kernel_size=3))\n            self.feature.add_module('f_bn3', nn.BatchNorm2d(256))\n            self.feature.add_module('f_drop3', nn.Dropout2d())\n            self.feature.add_module('f_pool3', nn.MaxPool2d(2))\n            self.feature.add_module('f_relu4', nn.ReLU(True))\n            self.feature.add_module('f_conv4', nn.Conv2d(256, 256, kernel_size=3))\n            self.feature.add_module('f_bn4', nn.BatchNorm2d(256))\n            self.feature.add_module('f_drop4', nn.Dropout2d())\n            self.feature.add_module('f_pool4', nn.MaxPool2d(2))\n            self.feature.add_module('f_relu4', nn.ReLU(True))\n            self.feature.add_module('f_conv5', nn.Conv2d(256, 512, kernel_size=3))\n            self.feature.add_module('f_bn5', nn.BatchNorm2d(512))\n            self.feature.add_module('f_drop5', nn.Dropout2d())\n            self.feature.add_module('f_pool5', nn.MaxPool2d(2))\n            self.feature.add_module('f_relu5', nn.ReLU(True))\n\n        self.class_classifier = nn.Sequential()\n        self.class_classifier.add_module('c_fc1', nn.Linear(self.ft_out_size, 100))\n        self.class_classifier.add_module('c_bn1', nn.BatchNorm1d(100))\n        self.class_classifier.add_module('c_relu1', nn.ReLU(True))\n        self.class_classifier.add_module('c_drop1', nn.Dropout2d())\n        self.class_classifier.add_module('c_fc2', nn.Linear(100, 100))\n        self.class_classifier.add_module('c_bn2', nn.BatchNorm1d(100))\n        self.class_classifier.add_module('c_relu2', nn.ReLU(True))\n        self.class_classifier.add_module('c_fc3', nn.Linear(100, 7))\n        self.class_classifier.add_module('c_softmax', nn.LogSoftmax(dim=1))\n\n        self.domain_classifier = nn.Sequential()\n        self.domain_classifier.add_module('d_fc1', nn.Linear(self.ft_out_size, 100))\n        self.domain_classifier.add_module('d_bn1', nn.BatchNorm1d(100))\n        self.domain_classifier.add_module('d_relu1', nn.ReLU(True))\n        self.domain_classifier.add_module('d_fc2', nn.Linear(100, 4))\n        self.domain_classifier.add_module('d_softmax', nn.LogSoftmax(dim=1))\n        \n\n\n    def forward(self, input_data, alpha):\n        input_data = input_data.expand(input_data.data.shape[0], 3, IMAGE_SIZE, IMAGE_SIZE)\n        feature = self.feature(input_data)\n        feature = feature.view(-1, self.ft_out_size)\n        reverse_feature = ReverseLayerF.apply(feature, alpha)\n        class_output = self.class_classifier(feature)\n        domain_output = self.domain_classifier(reverse_feature)\n\n        return class_output, domain_output","metadata":{"id":"LI3D9RwF0lcc","execution":{"iopub.status.busy":"2022-04-05T01:45:29.955564Z","iopub.execute_input":"2022-04-05T01:45:29.956073Z","iopub.status.idle":"2022-04-05T01:45:29.984685Z","shell.execute_reply.started":"2022-04-05T01:45:29.956017Z","shell.execute_reply":"2022-04-05T01:45:29.983803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test pipeline","metadata":{"id":"cjc2oINK088z"}},{"cell_type":"code","source":"def test(net, epoch):\n\n    # load data\n    img_transform_source = preprocess_fn()\n    img_transform_target = preprocess_fn(mu=(0.9566, 0.9566, 0.9566), std=(0.1752, 0.1752, 0.1752))\n\n\n    dataset_source, dataloader_source = prep_dataloader(\n        image_root=os.path.join(source_image_root, 'train_set'),\n        label_list=train_label_list,\n        img_transform=img_transform_source,\n        shuffle=False\n    )\n\n    dataset_target, dataloader_target = prep_dataloader(\n        image_root=os.path.join(target_image_root, 'test_set'),\n        img_transform=img_transform_target,\n        shuffle=False\n    )\n\n    net.eval()\n\n    if cuda:\n        net = net.cuda()\n\n    train_pths, train_preds = inference(net, dataloader_source, cuda=cuda, alpha=alpha)\n    train_results = pd.DataFrame({'id': train_pths, 'label': train_preds})\n    train_results_pth = os.path.join(output_root, '%s_train_epoch%s.csv' % (datetime.now().strftime(\"%m%d%Y\"), epoch))\n    train_results.to_csv(train_results_pth, index=False)\n\n    test_pths, test_preds = inference(net, dataloader_target, cuda=cuda, alpha=alpha)\n    test_results = pd.DataFrame({'id': test_pths, 'label': test_preds})\n    test_results_pth = os.path.join(output_root, '%s_test_epoch%s.csv' % (datetime.now().strftime(\"%m%d%Y\"), epoch))\n    test_results.to_csv(test_results_pth, index=False)\n\n    print('epoch: %d, accuracy of the train dataset: %f' % (epoch, compare(train_label_list, train_results_pth)))\n\ndef inference(net, dataloader, cuda=True, alpha=0.0):\n    preds = []\n    pths = []\n    for input_img, _,_, img_paths in dataloader: \n\n        if cuda:\n            input_img = input_img.cuda()\n\n        class_output, _ = net(input_data=input_img, alpha=alpha)\n        pred = class_output.data.max(1, keepdim=True)[1]\n        pths = pths + list(img_paths)\n        preds = preds + list(pred.squeeze(1).cpu().numpy())\n    return pths, preds\n\ndef compare(true_labels, predicted_labels):\n    combined_df = pd.read_csv(true_labels)\n    predicted_df = pd.read_csv(predicted_labels)\n\n    combined_df['label'] = combined_df['dir'].map(predicted_df.set_index('id')['label'])\n\n    true_labels = np.array(combined_df['label2'].to_list())\n    pred_labels = np.array(combined_df['label'].to_list())\n\n    return np.sum(true_labels == pred_labels) / len(true_labels)","metadata":{"id":"qmNvZ7O101l4","execution":{"iopub.status.busy":"2022-04-05T01:45:29.988557Z","iopub.execute_input":"2022-04-05T01:45:29.988758Z","iopub.status.idle":"2022-04-05T01:45:30.007264Z","shell.execute_reply.started":"2022-04-05T01:45:29.988732Z","shell.execute_reply":"2022-04-05T01:45:30.00648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training pipeline","metadata":{"id":"Av_ezC7pH7o7"}},{"cell_type":"code","source":"manual_seed = random.randint(1, 10000)\nrandom.seed(manual_seed)\ntorch.manual_seed(manual_seed)\n\n# load data\nimg_transform_source = preprocess_fn(aug=True)\nimg_transform_target = preprocess_fn(mu=(0.9566, 0.9566, 0.9566), std=(0.1752, 0.1752, 0.1752), aug=True)\n\n\ndataset_source, dataloader_source = prep_dataloader(\n    image_root=os.path.join(source_image_root, 'train_set'), \n    label_list=train_label_list,\n    img_transform=img_transform_source\n)\n\ndataset_target, dataloader_target = prep_dataloader(\n    image_root=os.path.join(target_image_root, 'test_set'),\n    img_transform=img_transform_target\n)\n\n# load model\nmy_net = CNNModel(model_name=model_name)\n\n# setup optimizer\noptimizer = optim.Adam(my_net.parameters(), lr=LR)\n\nloss_class = torch.nn.NLLLoss()\nloss_domain = torch.nn.NLLLoss()\n\nif cuda:\n    my_net = my_net.cuda()\n    loss_class = loss_class.cuda()\n    loss_domain = loss_domain.cuda()\n\nset_parameter_requires_grad(my_net, True)\n\n# training\nfor epoch in range(N_EPOCH):\n\n    len_dataloader = max(len(dataloader_source), len(dataloader_target))\n    data_source_iter = iter(dataloader_source)\n    data_target_iter = iter(dataloader_target)\n\n    i = 0\n    while i < len_dataloader:\n\n        p = float(i + epoch * len_dataloader) / N_EPOCH / len_dataloader\n        alpha = 2. / (1. + np.exp(-10 * p)) - 1\n\n        # training model using source data\n        data_source = data_source_iter.next()\n        s_img, s_label, s_domain_label, _ = data_source\n\n        my_net.zero_grad()\n        batch_size = len(s_label)\n\n        input_img = torch.FloatTensor(batch_size, 3, IMAGE_SIZE, IMAGE_SIZE)\n        class_label = torch.LongTensor(batch_size)\n        domain_label = torch.LongTensor(batch_size)\n\n        if cuda:\n            s_img = s_img.cuda()\n            s_label = s_label.cuda()\n            s_domain_label = s_domain_label.cuda()\n            input_img = input_img.cuda()\n            class_label = class_label.cuda()\n            domain_label = domain_label.cuda()\n\n        input_img.resize_as_(s_img).copy_(s_img)\n        class_label.resize_as_(s_label).copy_(s_label)\n        domain_label.resize_as_(s_domain_label).copy_(s_domain_label)\n\n        class_output, domain_output = my_net(input_data=input_img, alpha=alpha)\n        err_s_label = loss_class(class_output, class_label)\n        err_s_domain = loss_domain(domain_output, domain_label)\n\n        # training model using target data\n        if i == len(dataloader_target):\n            data_target_iter = iter(dataloader_target)\n        data_target = data_target_iter.next()\n        t_img, _, _, _ = data_target\n\n        batch_size = len(t_img) \n\n        input_img = torch.FloatTensor(batch_size, 3, IMAGE_SIZE, IMAGE_SIZE)\n        domain_label = torch.ones(batch_size) * 3.0\n        domain_label = domain_label.long()\n\n        if cuda:\n            t_img = t_img.cuda()\n            input_img = input_img.cuda()\n            domain_label = domain_label.cuda()\n\n        input_img.resize_as_(t_img).copy_(t_img)\n\n        _, domain_output = my_net(input_data=input_img, alpha=alpha)\n        err_t_domain = loss_domain(domain_output, domain_label)\n        err = err_t_domain + err_s_domain + err_s_label\n        err.backward()\n        optimizer.step()\n\n        i += 1\n\n        print('epoch: %d, [iter: %d / all %d], err_s_label: %f, err_s_domain: %f, err_t_domain: %f' \\\n            % (epoch, i, len_dataloader, err_s_label.cpu().data.numpy(),\n                err_s_domain.cpu().data.numpy(), err_t_domain.cpu().data.numpy()))\n\n    test(my_net, epoch)\n    my_net.train()\n\nprint('done')","metadata":{"id":"DOfxmvGjCWRz","outputId":"987cda66-3922-4fdc-cdeb-2b10feaff732","execution":{"iopub.status.busy":"2022-04-05T01:45:30.008422Z","iopub.execute_input":"2022-04-05T01:45:30.008881Z","iopub.status.idle":"2022-04-05T03:59:43.707931Z","shell.execute_reply.started":"2022-04-05T01:45:30.008766Z","shell.execute_reply":"2022-04-05T03:59:43.706493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use the same path as output_root\n!zip -r ./04092022_efficientnetb4_aug_run1.zip ./04092022_efficientnetb4_aug_run1/","metadata":{"execution":{"iopub.status.busy":"2022-04-05T03:59:43.710347Z","iopub.execute_input":"2022-04-05T03:59:43.710925Z","iopub.status.idle":"2022-04-05T03:59:44.634774Z","shell.execute_reply.started":"2022-04-05T03:59:43.710877Z","shell.execute_reply":"2022-04-05T03:59:44.633979Z"},"trusted":true},"execution_count":null,"outputs":[]}]}